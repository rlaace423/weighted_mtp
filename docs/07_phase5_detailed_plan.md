# Phase 5: Stageë³„ ë…ë¦½ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸ êµ¬í˜„ ê°€ì´ë“œ

## ë¬¸ì„œ ê°œìš”

ë³¸ ë¬¸ì„œëŠ” **Phase 5: Stageë³„ ë…ë¦½ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸ êµ¬í˜„**ì„ ìœ„í•œ ì‹¤í–‰ ê°€ì´ë“œì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë‹¨ì¼ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë°©ì‹ì„ ì™„ì „íˆ íê¸°í•˜ê³ , Critic Pre-training, Verifiable WMTP, Rho-1 Weighted Trainingì„ ê°ê° ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.

**ë²„ì „**: v2.0 (2025-01-17) - Stage ë¶„ë¦¬ ê¸°ë°˜ ì „ë©´ ì¬ì„¤ê³„
**ì„ í–‰ ì¡°ê±´**: Phase 3 (ë°ì´í„° íŒŒì´í”„ë¼ì¸), Phase 4 (Meta Adapter) ì™„ë£Œ
**ëª©í‘œ**: MLflow Projects í‘œì¤€ ê¸°ë°˜ Stageë³„ ë…ë¦½ ì‹¤í–‰ + Checkpoint Handoff

---

## Part 1: ê°œìš” ë° ë§¥ë½

### 1.1 Phase 5ì˜ ìœ„ì¹˜ì™€ ëª©ì 

Phase 5ëŠ” **í•™ìŠµ íŒŒì´í”„ë¼ì¸ì˜ ì‹¤í–‰ ë‹¨ìœ„ ë¶„ë¦¬**ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.

```
Phase 4 (model)  â†’  [Phase 5 (Stageë³„ ë…ë¦½ Runner)]  â†’  ì‹¤í—˜ ì‹¤í–‰
   Adapter êµ¬í˜„      Critic / Verifiable / Rho-1 ë¶„ë¦¬      ê° stage ë…ë¦½ ì‹¤í–‰
```

**í•µì‹¬ ì§ˆë¬¸**: ì–´ë–»ê²Œ Critic Pre-training, Verifiable Training, Rho-1 Trainingì„ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê³ , Stage ê°„ Checkpointë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì „ë‹¬í•  ê²ƒì¸ê°€?

### 1.2 Stage ë¶„ë¦¬ì˜ í•„ìš”ì„±

**ê¸°ì¡´ ë°©ì‹ì˜ ë¬¸ì œì ** (ë‹¨ì¼ `run_training_pipeline`):

| ë¬¸ì œ | ì„¤ëª… |
|------|------|
| **ì¬ì‹¤í–‰ ë¹„ìš©** | Stage 2ë§Œ ì¬ì‹¤í–‰í•˜ë ¤ë©´ Stage 1ë„ ë‹¤ì‹œ ì‹¤í–‰ í•„ìš” (2x ë¹„ìš©) |
| **ì‹¤í—˜ ì¶”ì  ë³µì¡ë„** | 1ê°œ MLflow runì— Stage 1+2 í˜¼ì¬, ë¶„ì„ ì–´ë ¤ì›€ |
| **Rho-1 ì§€ì› ë¶ˆê°€** | Ref model êµ¬ì¡°ê°€ ë‹¬ë¼ ë‹¨ì¼ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë¶ˆê°€ëŠ¥ |
| **Config ë³µì¡ë„** | training.stage1, training.stage2 í•œ íŒŒì¼ì— í˜¼ì¬ |
| **Checkpoint ì¬ì‚¬ìš©** | ë‚´ë¶€ ì „ë‹¬ë§Œ ê°€ëŠ¥, ì™¸ë¶€ ì¬ì‚¬ìš© ë¶ˆê°€ |

**Stage ë¶„ë¦¬ì˜ ì¥ì **:

| ì¥ì  | íš¨ê³¼ |
|------|------|
| **ë…ë¦½ ì‹¤í–‰** | Stage 2ë§Œ ì¬ì‹¤í–‰ â†’ ~50% ë¹„ìš© ì ˆê° (Critic ì¬ì‚¬ìš©) |
| **ì‹¤í—˜ ì¶”ì ** | ê° Stageë³„ MLflow run ë¶„ë¦¬ â†’ ëª…í™•í•œ ë¶„ì„ |
| **Rho-1 ì§€ì›** | ì™„ì „ ë…ë¦½ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ref model êµ¬ì¡° ìˆ˜ìš© |
| **Config ëª…í™•í™”** | configs/critic/, configs/verifiable/, configs/rho1/ ë¶„ë¦¬ |
| **Checkpoint ì¬ì‚¬ìš©** | MLflow artifactë¡œ ì €ì¥ â†’ ë‹¤ë¥¸ ì‹¤í—˜ì—ì„œ ë¡œë“œ ê°€ëŠ¥ |

### 1.3 MLflow Projects Best Practice (2024)

**í‘œì¤€ Multi-Step Workflow íŒ¨í„´**:

```python
# Step 1: Critic Pre-training
critic_run = mlflow.run(
    uri=".",
    entry_point="critic_training",
    parameters={"config": "configs/critic/critic.yaml"}
)
critic_run.wait()

# Artifact URI íšë“
critic_checkpoint = mlflow.get_artifact_uri(
    f"runs:/{critic_run.info.run_id}/checkpoints/checkpoint_best.pt"
)

# Step 2: Verifiable Training (Critic checkpoint ì‚¬ìš©)
verifiable_run = mlflow.run(
    uri=".",
    entry_point="verifiable_training",
    parameters={
        "config": "configs/verifiable/verifiable.yaml",
        "critic_checkpoint": critic_checkpoint
    }
)
```

**í•µì‹¬ ì›ì¹™**:
1. **ë…ë¦½ì„±**: ê° stepì€ entry_pointë¡œ ë…ë¦½ ì‹¤í–‰
2. **Artifact ì „ë‹¬**: ì´ì „ stepì˜ outputì„ MLflow artifactë¡œ ì €ì¥ â†’ ë‹¤ìŒ stepì—ì„œ ë¡œë“œ
3. **Config ë¶„ë¦¬**: ê° stepë³„ YAML config ë¶„ë¦¬
4. **Run ì¶”ì **: Parent-child run ê´€ê³„ë¡œ ì „ì²´ workflow ì¶”ì 

**LLM Continual Pre-training Pattern (2024)**:
- Stage 1 (Pre-training): Checkpoint ì €ì¥
- Stage 2 (Continued Training): Stage 1 checkpointë¡œ ì´ˆê¸°í™”
- Efficiency: ~2x ë¹„ìš© ì ˆê° (vs. from scratch)

### 1.4 ê¸°ëŒ€ íš¨ê³¼

1. **ì¬ì‹¤í–‰ íš¨ìœ¨**: Stage 2ë§Œ ì¬ì‹¤í–‰ ì‹œ Critic checkpoint ì¬ì‚¬ìš© â†’ 50% ë¹„ìš© ì ˆê°
2. **ì‹¤í—˜ ì¶”ì **: MLflowì—ì„œ Critic runê³¼ Verifiable run ëª…í™•íˆ êµ¬ë¶„
3. **Rho-1 ì§€ì›**: Ref model ê¸°ë°˜ ì™„ì „ ë…ë¦½ íŒŒì´í”„ë¼ì¸ êµ¬í˜„ ê°€ëŠ¥
4. **Config ê´€ë¦¬**: Stageë³„ ì„¤ì • ë¶„ë¦¬ë¡œ ê°€ë…ì„± í–¥ìƒ
5. **ì¬í˜„ì„±**: Checkpoint artifact URIë¡œ ì •í™•í•œ ì¬í˜„

---

## Part 2: í•µì‹¬ ì„¤ê³„ ê²°ì •

### 2.1 Decision 1: Stage ì™„ì „ ë¶„ë¦¬ ì•„í‚¤í…ì²˜

**ë¬¸ì œ ì¸ì‹**: ê¸°ì¡´ `run_training_pipeline()`ì€ Stage 1â†’2ë¥¼ í•œ í•¨ìˆ˜ì—ì„œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

**í•´ê²°ì±…**: ê° Stageë¥¼ ë…ë¦½ ì‹¤í–‰ ê°€ëŠ¥í•œ Runnerë¡œ ë¶„ë¦¬

**ì•„í‚¤í…ì²˜**:

```
src/weighted_mtp/pipelines/
â”œâ”€â”€ training.py              # ê³µí†µ í•¨ìˆ˜ (train_stage1, train_stage2, evaluate_stage)
â”œâ”€â”€ run_critic.py            # Stage 1 Runner (ë…ë¦½ ì‹¤í–‰)
â”œâ”€â”€ run_verifiable.py        # Stage 2 Runner (ë…ë¦½ ì‹¤í–‰, critic checkpoint ë¡œë“œ)
â”œâ”€â”€ run_rho1.py              # Rho-1 Runner (ë…ë¦½ ì‹¤í–‰, ref model ì‚¬ìš©)
â””â”€â”€ checkpoint_utils.py      # Checkpoint save/load ìœ í‹¸
```

**ì‹¤í–‰ ë°©ì‹**:

```bash
# Stage 1 (Critic Pre-training)
python -m weighted_mtp.pipelines.run_critic \
    --config configs/critic/critic.yaml

# Stage 2 (Verifiable WMTP)
python -m weighted_mtp.pipelines.run_verifiable \
    --config configs/verifiable/verifiable.yaml \
    --critic-checkpoint storage/checkpoints/critic/.../checkpoint_best.pt

# Rho-1 (ë…ë¦½)
python -m weighted_mtp.pipelines.run_rho1 \
    --config configs/rho1/rho1.yaml
```

**ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš©**:
- âœ… `training.py:train_stage1()` - Critic training ë¡œì§ ì¬ì‚¬ìš©
- âœ… `training.py:train_stage2()` - Verifiable training ë¡œì§ ì¬ì‚¬ìš©
- âœ… `value_weighting/` ì „ì²´ - TD error, weight builder ì¬ì‚¬ìš©
- âŒ `run_training_pipeline()` - ì‚­ì œ (ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë¶ˆí•„ìš”)

### 2.2 Decision 2: MLflow Artifact Checkpoint Handoff

**ë¬¸ì œ ì¸ì‹**: Stage ê°„ checkpoint ì „ë‹¬ ë©”ì»¤ë‹ˆì¦˜ í•„ìš”

**í•´ê²°ì±…**: MLflow Artifact + Local Path ë™ì‹œ ì§€ì›

**Checkpoint ì €ì¥** (Stage 1):
```python
# run_critic.py
checkpoint_path = Path(config.checkpoint.save_dir) / "checkpoint_best.pt"
save_checkpoint(adapter, optimizer, epoch, metrics, checkpoint_path)

# MLflow artifact ì—…ë¡œë“œ
mlflow.log_artifact(str(checkpoint_path), "checkpoints")
```

**Checkpoint ë¡œë“œ** (Stage 2):
```python
# run_verifiable.py
# Configì— ëª…ì‹œëœ ê²½ë¡œ (local or MLflow URI)
critic_checkpoint = config.experiment.critic_checkpoint

# Local path ë˜ëŠ” MLflow artifact URI ìë™ ê°ì§€
checkpoint = load_critic_checkpoint(critic_checkpoint, adapter, device)
```

**ì§€ì› ê²½ë¡œ í˜•ì‹**:
1. Local path: `storage/checkpoints/critic/.../checkpoint_best.pt`
2. MLflow artifact URI: `mlflow://8/{run_id}/artifacts/checkpoints/checkpoint_best.pt`

### 2.3 Decision 3: Config ê³„ì¸µ êµ¬ì¡° (defaults â†’ stage)

**ë¬¸ì œ ì¸ì‹**: Stageë³„ ì„¤ì • ì¤‘ë³µ ë° ê³µí†µ ì„¤ì • ê´€ë¦¬

**í•´ê²°ì±…**: 3-tier config hierarchy

```
configs/
â”œâ”€â”€ defaults.yaml          # Tier 1: ê³µí†µ (models, storage, mlflow, runtime)
â”œâ”€â”€ critic/
â”‚   â”œâ”€â”€ critic.yaml       # Tier 2: Stage 1 ì „ìš© (defaults ìƒì†)
â”‚   â””â”€â”€ critic_local.yaml # Tier 3: Local test override
â”œâ”€â”€ verifiable/
â”‚   â”œâ”€â”€ verifiable.yaml   # Tier 2: Stage 2 ì „ìš© (defaults ìƒì† + critic_checkpoint)
â”‚   â””â”€â”€ verifiable_local.yaml
â””â”€â”€ rho1/
    â”œâ”€â”€ rho1.yaml         # Tier 2: Rho-1 ì „ìš© (ref model í¬í•¨)
    â””â”€â”€ rho1_local.yaml
```

**Merge ìˆœì„œ** (OmegaConf):
```python
defaults = OmegaConf.load("configs/defaults.yaml")
config = OmegaConf.load("configs/critic/critic.yaml")
config = OmegaConf.merge(defaults, config)  # Tier 1 + Tier 2
```

**ì¥ì **:
- ê³µí†µ ì„¤ì • (models, mlflow) í•œ ê³³ì—ì„œ ê´€ë¦¬
- Stageë³„ ì°¨ì´ì ë§Œ ëª…ì‹œ (critic: n_epochs=0.5, verifiable: n_epochs=2.5)
- Local test configë¡œ micro model override ê°€ëŠ¥

### 2.4 Decision 4: Rho-1 ë³„ë„ íŒŒì´í”„ë¼ì¸

**ë¬¸ì œ ì¸ì‹**: Rho-1ì€ ref model í•„ìš” â†’ êµ¬ì¡°ê°€ Verifiableê³¼ ë‹¤ë¦„

**í•´ê²°ì±…**: ì™„ì „ ë…ë¦½ íŒŒì´í”„ë¼ì¸ `run_rho1.py`

**Rho-1 êµ¬ì¡° ì°¨ì´**:

| ì¸¡ë©´ | Verifiable | Rho-1 |
|------|-----------|-------|
| **Critic ì‚¬ìš©** | âœ… Stage 1 checkpoint ë¡œë“œ | âŒ ë¶ˆì‚¬ìš© |
| **Ref model** | âŒ ë¶ˆí•„ìš” | âœ… í•„ìˆ˜ (excess loss ê³„ì‚°) |
| **Weight ê³„ì‚°** | TD error ê¸°ë°˜ | Excess loss ê¸°ë°˜ (policy vs ref) |
| **Pipeline** | Stage 1 â†’ Stage 2 | ë…ë¦½ ì‹¤í–‰ (1-stage) |

**Config ì°¨ì´**:
```yaml
# configs/rho1/rho1.yaml
models:
  policy:
    name: meta-llama-mtp
    path: storage/models_v2/meta-llama-mtp

  reference:  # â­ Rho-1 ì „ìš©
    name: ref-sheared-llama-2.7b
    path: storage/models_v2/ref-sheared-llama-2.7b

training:
  use_reference_model: true
  alpha: 0.3  # SLM-ratio (excess loss threshold)
  weight_strategy: rho1  # vs. verifiable
```

---

## Part 3: Value Weighting ëª¨ë“ˆ ì„¤ê³„

### 3.1 TD Error ê³„ì‚° (í‘œì¤€ Temporal Difference)

**ì´ë¡ ì  ë°°ê²½**:

í‘œì¤€ TD(0) ê³µì‹ (Sutton & Barto):
```python
# Intermediate tokens (k < T): Bootstrapping
Î´_k = r_k + Î³V(s_k) - V(s_{k-1})
    = Î³V(s_k) - V(s_{k-1})  # r_k = 0 (ì¤‘ê°„ í† í° ë³´ìƒ ì—†ìŒ)

# Terminal token (k = T): Direct reward
Î´_T = R - V(s_{T-1})  # V(terminal) = 0 ê°€ì •
```

**êµ¬í˜„ ìš”êµ¬ì‚¬í•­**:

```python
def compute_td_errors(
    value_logits: torch.Tensor,  # [batch, seq, 1]
    rewards: torch.Tensor,        # [batch] - Binary: 0 or 1
    gamma: float = 1.0,           # í• ì¸ìœ¨ (undiscounted)
    attention_mask: torch.Tensor = None,  # [batch, seq]
) -> torch.Tensor:
    """í‘œì¤€ TD error ê³„ì‚°

    TD errorëŠ” Î´_t = V(s_{t+1}) - V(s_t) (Î³=1.0)ë¡œ ê³„ì‚°ë˜ë©°,
    "ì´ í† í°ì´ ì„±ê³µ í™•ë¥ ì„ ì–¼ë§ˆë‚˜ ë³€í™”ì‹œì¼°ëŠ”ê°€ (Î”P)"ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

    Returns:
        td_errors: [batch, seq] TD error (Intermediate + Terminal)
    """
```

**í•µì‹¬ ë¡œì§**:
```python
# Value squeeze
values = value_logits.squeeze(-1)  # [batch, seq]

# Bootstrapping: V(s_k) - V(s_{k-1})
value_next = values[:, 1:]  # [batch, seq-1]
value_current = values[:, :-1]  # [batch, seq-1]
td_errors_intermediate = gamma * value_next - value_current

# Terminal: R - V(s_{T-1})
values_terminal = values[:, -1]  # [batch]
td_errors_terminal = rewards - values_terminal

# Combine
td_errors = torch.cat([td_errors_intermediate, td_errors_terminal.unsqueeze(1)], dim=1)
```

**ê²€ì¦ ê¸°ì¤€**:
- [ ] Intermediate TD error ê³„ì‚° ì •í™•: `gamma * V_next - V_current`
- [ ] Terminal TD error ê³„ì‚° ì •í™•: `reward - V_terminal`
- [ ] Binary reward [0, 1] í™˜ê²½ì—ì„œ TD error bounded [-1, 1]
- [ ] Padding mask ì •ìƒ ë™ì‘

### 3.2 Weight Builder (Exponential Weighting)

**ì´ë¡ ì  ë°°ê²½**:

IQL/AWR Exponential Weighting:
```python
weight = exp(td_error / Î²)
weight = clamp(weight, min=0.1, max=5.0)  # Conservative clipping
```

**ì§ê´€**:
- Positive TD error (td > 0): weight > 1 â†’ ì¤‘ìš” í† í° ê°•í™”
- Negative TD error (td < 0): weight < 1 â†’ ë¹„ì¤‘ìš” í† í° down-weight
- Incorrect ìƒ˜í”Œ: reward=0, value>0 â†’ td<0 â†’ weight<1 (ìë™ í•„í„°ë§)

**êµ¬í˜„ ìš”êµ¬ì‚¬í•­**:

```python
def build_weights(
    td_errors: torch.Tensor,  # [batch, seq]
    beta: float = 0.9,         # Temperature parameter
    min_weight: float = 0.1,   # Conservative minimum
    max_weight: float = 5.0,   # Conservative maximum
) -> torch.Tensor:
    """TD error ê¸°ë°˜ exponential weighting

    Returns:
        weights: [batch, seq] Token-level weights
    """
    weights = torch.exp(td_errors / beta)
    weights = torch.clamp(weights, min=min_weight, max=max_weight)
    return weights
```

**ê²€ì¦ ê¸°ì¤€**:
- [ ] Exponential weighting: `exp(td / beta)` ì •í™• ê³„ì‚°
- [ ] Clipping: min=0.1, max=5.0 ì ìš© í™•ì¸
- [ ] Beta sensitivity: Î²â†“ â†’ weight ì°¨ì´ ì¦ê°€
- [ ] Gradient ê³„ì‚° ê°€ëŠ¥ (requires_grad=True ì§€ì›)

### 3.3 Metrics (Statistics Computation)

**TD Error Statistics**:

```python
def compute_td_stats(td_errors: torch.Tensor) -> dict[str, float]:
    """TD error í†µê³„ ê³„ì‚°

    Returns:
        {
            "td_mean": float,
            "td_std": float,
            "td_min": float,
            "td_max": float,
        }
    """
```

**Weight Statistics**:

```python
def compute_weight_stats(weights: torch.Tensor) -> dict[str, float]:
    """Weight í†µê³„ ê³„ì‚°

    Returns:
        {
            "weight_mean": float,
            "weight_std": float,
            "weight_min": float,
            "weight_max": float,
            "weight_entropy": float,  # Distribution entropy
        }
    """
```

**ê²€ì¦ ê¸°ì¤€**:
- [ ] í‰ê· /í‘œì¤€í¸ì°¨ ê³„ì‚° ì •í™•
- [ ] Weight entropy ê³„ì‚° (0-log(seq_len) ë²”ìœ„)
- [ ] Padding mask ê³ ë ¤ (ìœ íš¨ í† í°ë§Œ í†µê³„ ê³„ì‚°)

---

## Part 4: Config ë¶„ë¦¬ êµ¬ì¡°

### 4.1 defaults.yaml (ê³µí†µ ì„¤ì •)

**ì—­í• **: ëª¨ë“  stageì—ì„œ ìƒì†í•˜ëŠ” ê¸°ë³¸ê°’

**ë‚´ìš©**:

```yaml
# ê³µí†µ ì„¤ì • (ì¥ë¹„, ìŠ¤í† ë¦¬ì§€, ëª¨ë¸ íŒŒë¼ë¯¸í„° ìŠ¤ëƒ…ìƒ·)

project:
  name: weighted-mtp
  version: "2.0.0"

storage:
  root: storage
  models_dir: storage/models_v2
  datasets_dir: storage/datasets_v2
  checkpoints_dir: storage/checkpoints  # â­ Checkpoint ì €ì¥ ê²½ë¡œ

models:
  policy:
    name: meta-llama-mtp
    path: storage/models_v2/meta-llama-mtp
    params:
      dim: 4096
      n_layers: 32
      n_heads: 32
      n_future_tokens: 4
      intermediate_size: 11008
      rope_theta: 10000.0
      vocab_size: 32000
    dtype: float16

  reference:
    name: ref-sheared-llama-2.7b
    path: storage/models_v2/ref-sheared-llama-2.7b
    dtype: float16
    tokenizer_shared_with: meta-llama-mtp

  reward:
    name: starling-rm-7b
    path: storage/models_v2/starling-rm-7b
    dtype: bfloat16
    status: optional

runtime:
  device: cuda
  seed: 42
  mixed_precision: true

mlflow:
  tracking_uri: "http://13.50.240.176"  # EC2 MLflow Server (Basic Auth)
  experiment: "weighted-mtp/production"
  s3_artifacts: "s3://wmtp/mlflow-artifacts"

logging:
  level: INFO
  format: "%(asctime)s [%(levelname)s] %(name)s: %(message)s"
```

**ìˆ˜ì • ê¸ˆì§€**: ì´ íŒŒì¼ì€ ëª¨ë“  stageì—ì„œ ê³µìœ ë˜ë¯€ë¡œ ë³€ê²½ ì‹œ ì˜í–¥ ë²”ìœ„ í™•ì¸ í•„ìš”

### 4.2 configs/critic/critic.yaml (Stage 1 ì „ìš©)

**ì—­í• **: Critic (Value head) pre-training ë…ë¦½ ì‹¤í–‰

**íŒŒì¼ ê²½ë¡œ**: `configs/critic/critic.yaml`

**ë‚´ìš©**:

```yaml
# Critic Pre-training (Stage 1)
experiment:
  name: critic-pretrain
  description: "Value head pretraining for TD error calculation"
  stage: critic
  tags:
    - critic
    - value-head
    - stage1

dataset:
  name: codecontests
  train: storage/datasets_v2/codecontests/processed/train.jsonl
  validation: storage/datasets_v2/codecontests/processed/valid.jsonl
  max_length: 2048

data_sampling:
  # Stage 1: Value Head Pretrain (is_correct ê· í˜• ìƒ˜í”Œë§)
  # ëª©ì : Binary classification (correct vs incorrect)
  n_samples: 30000  # Effective: 30K samples (15K correct + 15K incorrect)
  balance_correct: true
  correct_ratio: 0.5
  difficulty_range: [1, 11]  # ì „ì²´ ë‚œì´ë„
  seed: 42

training:
  n_epochs: 0.5
  batch_size: 8
  learning_rate: 1.0e-4
  loss_type: mse  # mse or huber

  # Logging & Evaluation
  log_interval: 10      # 10 stepë§ˆë‹¤ train loss ì¶œë ¥
  eval_interval: 100    # 100 stepë§ˆë‹¤ validation í‰ê°€
  save_checkpoint_every: 0.5  # 0.5 epochë§ˆë‹¤ checkpoint ì €ì¥

checkpoint:
  save_dir: storage/checkpoints/critic/${experiment.name}
  save_best: true   # Best validation loss checkpoint
  save_final: true  # Final checkpoint
```

**Local test config**: `configs/critic/critic_local.yaml`

```yaml
# Critic Local Test (Micro model)
experiment:
  name: critic-pretrain-local
  stage: critic
  tags:
    - critic
    - local
    - micro-model

models:
  policy:
    name: micro-mtp
    path: storage/models_v2/micro-mtp  # Override (micro model)

data_sampling:
  n_samples: 100  # ì†ŒëŸ‰ ìƒ˜í”Œ

training:
  n_epochs: 0.1
  batch_size: 2
```

### 4.3 configs/verifiable/verifiable.yaml (Stage 2 ì „ìš©)

**ì—­í• **: Verifiable WMTP ë…ë¦½ ì‹¤í–‰ (Critic checkpoint ë¡œë“œ)

**íŒŒì¼ ê²½ë¡œ**: `configs/verifiable/verifiable.yaml`

**ë‚´ìš©**:

```yaml
# Verifiable WMTP (Stage 2)
experiment:
  name: verifiable-wmtp
  description: "WMTP with TD error-based token weighting"
  stage: verifiable
  tags:
    - verifiable
    - wmtp
    - stage2

  # â­ Stage 1 checkpoint ê²½ë¡œ (í•„ìˆ˜)
  critic_checkpoint: storage/checkpoints/critic/critic-pretrain/checkpoint_best.pt
  # ë˜ëŠ” MLflow artifact URI:
  # critic_checkpoint: mlflow://8/{run_id}/artifacts/checkpoints/checkpoint_best.pt

dataset:
  name: codecontests
  train: storage/datasets_v2/codecontests/processed/train.jsonl
  validation: storage/datasets_v2/codecontests/processed/valid.jsonl
  max_length: 2048

data_sampling:
  # Stage 2: Weighted Training (Curriculum Learning)
  n_samples: 100000  # Effective: 300K samples (100K Ã— 3 epochs)
  balance_correct: true
  correct_ratio: 0.5
  curriculum_learning: true
  difficulty_bins:
    low: [1, 3]      # ì‰¬ìš´ ë¬¸ì œ
    medium: [4, 7]   # ì¤‘ê°„ ë¬¸ì œ
    high: [8, 11]    # ì–´ë ¤ìš´ ë¬¸ì œ
  curriculum_schedule:
    - epoch_range: [0.0, 0.3]
      difficulty_weights: {low: 0.7, medium: 0.3, high: 0.0}
    - epoch_range: [0.3, 0.7]
      difficulty_weights: {low: 0.3, medium: 0.6, high: 0.1}
    - epoch_range: [0.7, 1.0]
      difficulty_weights: {low: 0.1, medium: 0.5, high: 0.4}
  seed: 42

training:
  n_epochs: 2.5
  batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 1.0e-5

  # Weighted MTP
  beta: 0.9               # Exponential weighting temperature
  value_coef: 0.5         # Value loss coefficient (Critic Continual Learning)
  max_grad_norm: 0.5      # Gradient clipping
  loss_type: mse          # Value loss type
  weight_clip_min: 0.1    # Conservative weight minimum
  weight_clip_max: 5.0    # Conservative weight maximum

  # Logging & Evaluation
  log_interval: 10
  eval_interval: 100
  save_checkpoint_every: 1.0

checkpoint:
  save_dir: storage/checkpoints/verifiable/${experiment.name}
  save_best: true
  save_final: true
```

### 4.4 configs/rho1/rho1.yaml (Rho-1 ì „ìš©)

**ì—­í• **: Rho-1 Weighted Training (Ref model í•„ìš”)

**íŒŒì¼ ê²½ë¡œ**: `configs/rho1/rho1.yaml`

**ë‚´ìš©**:

```yaml
# Rho-1 Weighted Training
experiment:
  name: rho1-wmtp
  description: "Rho-1 style token weighting with reference model"
  stage: rho1
  tags:
    - rho1
    - reference-based
    - wmtp

models:
  # â­ Reference model í•„ìˆ˜
  policy:
    name: meta-llama-mtp
    path: storage/models_v2/meta-llama-mtp

  reference:
    name: ref-sheared-llama-2.7b
    path: storage/models_v2/ref-sheared-llama-2.7b
    # Reference modelë¡œ excess loss ê³„ì‚°

dataset:
  name: codecontests
  train: storage/datasets_v2/codecontests/processed/train.jsonl
  validation: storage/datasets_v2/codecontests/processed/valid.jsonl
  max_length: 2048

data_sampling:
  n_samples: 100000
  balance_correct: true
  correct_ratio: 0.5
  seed: 42

training:
  n_epochs: 3.0
  batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 1.0e-5

  # Rho-1 specific
  use_reference_model: true
  alpha: 0.3  # SLM-ratio (excess loss threshold)
  weight_strategy: rho1  # vs. verifiable

  # Logging & Evaluation
  log_interval: 10
  eval_interval: 100
  save_checkpoint_every: 1.0

checkpoint:
  save_dir: storage/checkpoints/rho1/${experiment.name}
  save_best: true
  save_final: true
```

---

## Part 5: Pipeline Runners ì„¤ê³„

### 5.1 run_critic.py (Stage 1 Runner)

**íŒŒì¼ ê²½ë¡œ**: `src/weighted_mtp/pipelines/run_critic.py`

**ì—­í• **: Critic pre-training ë…ë¦½ ì‹¤í–‰

**í•µì‹¬ êµ¬ì¡°**:

```python
"""Critic Pre-training Runner (Stage 1)

ë…ë¦½ ì‹¤í–‰:
    python -m weighted_mtp.pipelines.run_critic --config configs/critic/critic.yaml
"""

import logging
from pathlib import Path

import mlflow
import torch
from omegaconf import OmegaConf

from weighted_mtp.models import load_adapter, load_tokenizer
from weighted_mtp.data import load_dataset, create_dataloader
from weighted_mtp.pipelines.training import train_stage1, evaluate_stage
from weighted_mtp.pipelines.checkpoint_utils import save_checkpoint
from weighted_mtp.runtime.distributed import is_main_process

logger = logging.getLogger(__name__)


def run_critic_training(config_path: str, **override_params):
    """Critic pre-training ì‹¤í–‰

    Args:
        config_path: configs/critic/critic.yaml
        override_params: CLI overrides

    Returns:
        metrics: Final metrics
        checkpoint_path: Best checkpoint path
    """
    # 1. Config ë¡œë”© (defaults + critic config merge)
    defaults = OmegaConf.load("configs/defaults.yaml")
    config = OmegaConf.load(config_path)
    config = OmegaConf.merge(defaults, config, override_params)

    # 2. MLflow ì´ˆê¸°í™” (Rank 0 only)
    if is_main_process():
        mlflow.set_tracking_uri(config.mlflow.tracking_uri)
        mlflow.set_experiment(config.mlflow.experiment)

        with mlflow.start_run(run_name=config.experiment.name, tags=config.experiment.tags):
            mlflow.log_params(OmegaConf.to_container(config, resolve=True))

            # 3. Resource ë¡œë”©
            device = torch.device(config.runtime.device)
            adapter = load_adapter(config.models.policy, device)
            tokenizer = load_tokenizer(config.models.policy)

            train_dataset = load_dataset(config.dataset.train, config.data_sampling)
            val_dataset = load_dataset(config.dataset.validation, use_full=True)

            train_loader = create_dataloader(train_dataset, tokenizer, config.training.batch_size)
            val_loader = create_dataloader(val_dataset, tokenizer, config.training.batch_size, shuffle=False)

            # 4. Optimizer (Value head only)
            optimizer = torch.optim.Adam(
                adapter.value_head.parameters(),
                lr=config.training.learning_rate
            )

            # 5. Training loop
            logger.info("=== Critic Pre-training (Stage 1) ===")

            best_val_loss = float('inf')
            best_checkpoint_path = None

            n_epochs = int(config.training.n_epochs) + 1
            for epoch in range(n_epochs):
                # Train
                train_metrics = train_stage1(
                    adapter=adapter,
                    dataloader=train_loader,
                    optimizer=optimizer,
                    config=config.training,
                    device=device,
                )

                # Evaluate
                val_metrics = evaluate_stage(
                    adapter=adapter,
                    dataloader=val_loader,
                    config=config.training,
                    device=device,
                    stage="stage1"
                )

                # Log metrics to MLflow
                mlflow.log_metrics({
                    **{f"train/{k}": v for k, v in train_metrics.items()},
                    **{f"val/{k}": v for k, v in val_metrics.items()},
                }, step=epoch)

                # Save checkpoint
                checkpoint_dir = Path(config.checkpoint.save_dir)
                checkpoint_path = checkpoint_dir / f"checkpoint_epoch_{epoch}.pt"
                save_checkpoint(adapter, optimizer, epoch, train_metrics, val_metrics, checkpoint_path)

                # Best checkpoint
                if config.checkpoint.save_best and val_metrics["val_loss"] < best_val_loss:
                    best_val_loss = val_metrics["val_loss"]
                    best_checkpoint_path = checkpoint_dir / "checkpoint_best.pt"
                    save_checkpoint(adapter, optimizer, epoch, train_metrics, val_metrics, best_checkpoint_path)
                    logger.info(f"âœ… Best checkpoint saved: {best_checkpoint_path} (val_loss={best_val_loss:.4f})")

            # 6. Final checkpoint
            if config.checkpoint.save_final:
                final_path = checkpoint_dir / "checkpoint_final.pt"
                save_checkpoint(adapter, optimizer, n_epochs - 1, train_metrics, val_metrics, final_path)

            # 7. MLflow artifact ì—…ë¡œë“œ
            mlflow.log_artifact(str(best_checkpoint_path), "checkpoints")

            logger.info(f"ğŸ‰ Critic pre-training ì™„ë£Œ! Best checkpoint: {best_checkpoint_path}")

            return val_metrics, best_checkpoint_path


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Critic Pre-training (Stage 1)")
    parser.add_argument("--config", required=True, help="Config path (e.g., configs/critic/critic.yaml)")
    parser.add_argument("--run-name", help="MLflow run name override")
    parser.add_argument("--device", help="Device override (cuda/cpu)")
    args = parser.parse_args()

    overrides = {}
    if args.run_name:
        overrides["experiment.name"] = args.run_name
    if args.device:
        overrides["runtime.device"] = args.device

    run_critic_training(args.config, **overrides)
```

**í•µì‹¬ í¬ì¸íŠ¸**:
1. **Config merge**: defaults.yaml + critic.yaml
2. **MLflow run ìƒì„±**: run_name, tags ì„¤ì •
3. **Value headë§Œ í•™ìŠµ**: `adapter.value_head.parameters()`
4. **Best checkpoint ì¶”ì **: `val_loss` ìµœì†Œí™”
5. **Artifact ì—…ë¡œë“œ**: MLflowì— checkpoint ì €ì¥

### 5.2 run_verifiable.py (Stage 2 Runner)

**íŒŒì¼ ê²½ë¡œ**: `src/weighted_mtp/pipelines/run_verifiable.py`

**ì—­í• **: Verifiable WMTP ë…ë¦½ ì‹¤í–‰ (Critic checkpoint ë¡œë“œ)

**í•µì‹¬ êµ¬ì¡°**:

```python
"""Verifiable WMTP Runner (Stage 2)

ë…ë¦½ ì‹¤í–‰:
    python -m weighted_mtp.pipelines.run_verifiable \
        --config configs/verifiable/verifiable.yaml \
        --critic-checkpoint storage/checkpoints/critic/.../checkpoint_best.pt
"""

import logging
from pathlib import Path

import mlflow
import torch
from omegaconf import OmegaConf

from weighted_mtp.models import load_adapter, load_tokenizer
from weighted_mtp.data import load_dataset, create_dataloader
from weighted_mtp.pipelines.training import train_stage2, evaluate_stage
from weighted_mtp.pipelines.checkpoint_utils import save_checkpoint, load_critic_checkpoint
from weighted_mtp.runtime.distributed import is_main_process

logger = logging.getLogger(__name__)


def run_verifiable_training(config_path: str, critic_checkpoint: str = None, **override_params):
    """Verifiable WMTP ì‹¤í–‰

    Args:
        config_path: configs/verifiable/verifiable.yaml
        critic_checkpoint: Critic checkpoint ê²½ë¡œ (CLI override)
        override_params: ì¶”ê°€ overrides

    Returns:
        metrics: Final metrics
        checkpoint_path: Best checkpoint path
    """
    # 1. Config ë¡œë”©
    defaults = OmegaConf.load("configs/defaults.yaml")
    config = OmegaConf.load(config_path)
    config = OmegaConf.merge(defaults, config, override_params)

    # CLI override critic checkpoint
    if critic_checkpoint:
        config.experiment.critic_checkpoint = critic_checkpoint

    # 2. Critic checkpoint ê²½ë¡œ ê²€ì¦
    if not config.experiment.critic_checkpoint:
        raise ValueError(
            "critic_checkpointê°€ í•„ìš”í•©ë‹ˆë‹¤!\n"
            "  1) Configì— ëª…ì‹œ: experiment.critic_checkpoint\n"
            "  2) CLI ì¸ì: --critic-checkpoint <path>"
        )

    # 3. MLflow ì´ˆê¸°í™”
    if is_main_process():
        mlflow.set_tracking_uri(config.mlflow.tracking_uri)
        mlflow.set_experiment(config.mlflow.experiment)

        with mlflow.start_run(run_name=config.experiment.name, tags=config.experiment.tags):
            mlflow.log_params(OmegaConf.to_container(config, resolve=True))
            mlflow.log_param("critic_checkpoint", config.experiment.critic_checkpoint)

            # 4. Resource ë¡œë”©
            device = torch.device(config.runtime.device)
            adapter = load_adapter(config.models.policy, device)
            tokenizer = load_tokenizer(config.models.policy)

            # â­ Critic checkpoint ë¡œë“œ (Value head ì´ˆê¸°í™”)
            logger.info(f"Loading critic checkpoint: {config.experiment.critic_checkpoint}")
            load_critic_checkpoint(config.experiment.critic_checkpoint, adapter, device)
            logger.info("âœ… Critic checkpoint loaded successfully")

            # Dataset ë¡œë”©
            train_dataset = load_dataset(config.dataset.train, config.data_sampling)
            val_dataset = load_dataset(config.dataset.validation, use_full=True)

            train_loader = create_dataloader(train_dataset, tokenizer, config.training.batch_size)
            val_loader = create_dataloader(val_dataset, tokenizer, config.training.batch_size, shuffle=False)

            # 5. Optimizer (ì „ì²´ íŒŒë¼ë¯¸í„°)
            optimizer = torch.optim.Adam(
                adapter.parameters(),
                lr=config.training.learning_rate
            )

            # 6. Training loop
            logger.info("=== Verifiable WMTP (Stage 2) ===")

            best_val_loss = float('inf')
            best_checkpoint_path = None

            n_epochs = int(config.training.n_epochs) + 1
            for epoch in range(n_epochs):
                # Train
                train_metrics = train_stage2(
                    adapter=adapter,
                    dataloader=train_loader,
                    optimizer=optimizer,
                    config=config.training,
                    device=device,
                )

                # Evaluate
                val_metrics = evaluate_stage(
                    adapter=adapter,
                    dataloader=val_loader,
                    config=config.training,
                    device=device,
                    stage="stage2"
                )

                # Log metrics
                mlflow.log_metrics({
                    **{f"train/{k}": v for k, v in train_metrics.items()},
                    **{f"val/{k}": v for k, v in val_metrics.items()},
                }, step=epoch)

                # Save checkpoint
                checkpoint_dir = Path(config.checkpoint.save_dir)
                checkpoint_path = checkpoint_dir / f"checkpoint_epoch_{epoch}.pt"
                save_checkpoint(adapter, optimizer, epoch, train_metrics, val_metrics, checkpoint_path)

                # Best checkpoint
                if config.checkpoint.save_best and val_metrics["val_loss"] < best_val_loss:
                    best_val_loss = val_metrics["val_loss"]
                    best_checkpoint_path = checkpoint_dir / "checkpoint_best.pt"
                    save_checkpoint(adapter, optimizer, epoch, train_metrics, val_metrics, best_checkpoint_path)
                    logger.info(f"âœ… Best checkpoint saved: {best_checkpoint_path} (val_loss={best_val_loss:.4f})")

            # 7. Final checkpoint
            if config.checkpoint.save_final:
                final_path = checkpoint_dir / "checkpoint_final.pt"
                save_checkpoint(adapter, optimizer, n_epochs - 1, train_metrics, val_metrics, final_path)

            # 8. MLflow artifact ì—…ë¡œë“œ
            mlflow.log_artifact(str(best_checkpoint_path), "checkpoints")

            logger.info(f"ğŸ‰ Verifiable WMTP ì™„ë£Œ! Best checkpoint: {best_checkpoint_path}")

            return val_metrics, best_checkpoint_path


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Verifiable WMTP (Stage 2)")
    parser.add_argument("--config", required=True, help="Config path (e.g., configs/verifiable/verifiable.yaml)")
    parser.add_argument("--critic-checkpoint", help="Critic checkpoint path (local or MLflow URI)")
    parser.add_argument("--run-name", help="MLflow run name override")
    args = parser.parse_args()

    overrides = {}
    if args.run_name:
        overrides["experiment.name"] = args.run_name

    run_verifiable_training(args.config, critic_checkpoint=args.critic_checkpoint, **overrides)
```

**í•µì‹¬ ì°¨ì´ì ** (vs. run_critic.py):
1. **Critic checkpoint ë¡œë“œ**: `load_critic_checkpoint()` í˜¸ì¶œ
2. **ì „ì²´ íŒŒë¼ë¯¸í„° í•™ìŠµ**: `adapter.parameters()` (MTP heads + Value head)
3. **train_stage2 ì‚¬ìš©**: Weighted MTP + Critic Continual Learning

### 5.3 run_rho1.py (Rho-1 Runner)

**íŒŒì¼ ê²½ë¡œ**: `src/weighted_mtp/pipelines/run_rho1.py`

**ì—­í• **: Rho-1 Weighted Training (Ref model ì‚¬ìš©)

**í•µì‹¬ êµ¬ì¡°**:

```python
"""Rho-1 Weighted Training Runner

ë…ë¦½ ì‹¤í–‰:
    python -m weighted_mtp.pipelines.run_rho1 --config configs/rho1/rho1.yaml
"""

def run_rho1_training(config_path: str, **override_params):
    """Rho-1 training ì‹¤í–‰

    Args:
        config_path: configs/rho1/rho1.yaml

    Returns:
        metrics: Final metrics
        checkpoint_path: Best checkpoint path
    """
    # 1. Config ë¡œë”©
    defaults = OmegaConf.load("configs/defaults.yaml")
    config = OmegaConf.load(config_path)
    config = OmegaConf.merge(defaults, config, override_params)

    # 2. MLflow ì´ˆê¸°í™”
    if is_main_process():
        mlflow.set_tracking_uri(config.mlflow.tracking_uri)
        mlflow.set_experiment(config.mlflow.experiment)

        with mlflow.start_run(run_name=config.experiment.name, tags=config.experiment.tags):
            # 3. Resource ë¡œë”©
            device = torch.device(config.runtime.device)

            # â­ Policy + Reference model ë¡œë”©
            policy_adapter = load_adapter(config.models.policy, device)
            ref_model = load_reference_model(config.models.reference, device)

            tokenizer = load_tokenizer(config.models.policy)

            # Dataset ë¡œë”©
            train_dataset = load_dataset(config.dataset.train, config.data_sampling)
            val_dataset = load_dataset(config.dataset.validation, use_full=True)

            train_loader = create_dataloader(train_dataset, tokenizer, config.training.batch_size)
            val_loader = create_dataloader(val_dataset, tokenizer, config.training.batch_size, shuffle=False)

            # 4. Optimizer
            optimizer = torch.optim.Adam(
                policy_adapter.parameters(),
                lr=config.training.learning_rate
            )

            # 5. Training (Rho-1 logic)
            logger.info("=== Rho-1 Weighted Training ===")

            # Rho-1 specific training loop
            # - Compute excess loss (policy vs ref)
            # - Build weights based on excess loss
            # - Weighted CE loss

            # (êµ¬í˜„ ìƒëµ - train_rho1() í•¨ìˆ˜ í˜¸ì¶œ)
```

**í•µì‹¬ ì°¨ì´ì **:
1. **Ref model ë¡œë”©**: `load_reference_model()` ì‚¬ìš©
2. **Excess loss ê³„ì‚°**: Policy loss - Ref loss
3. **Weight ì „ëµ**: `weight_strategy: rho1` (vs. verifiable)

### 5.4 ê³µí†µ í•¨ìˆ˜ ì¬ì‚¬ìš©

**ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜** (`pipelines/training.py`):

| í•¨ìˆ˜ | ì—­í•  | ì‚¬ìš© ìœ„ì¹˜ |
|------|------|----------|
| `train_stage1()` | Critic training ë¡œì§ | run_critic.py |
| `train_stage2()` | Verifiable training ë¡œì§ | run_verifiable.py |
| `evaluate_stage()` | Validation í‰ê°€ | ëª¨ë“  runner |

**ë³€ê²½ ì—†ìŒ**: ê¸°ì¡´ Phase 5ì—ì„œ êµ¬í˜„ëœ í•¨ìˆ˜ ê·¸ëŒ€ë¡œ ì¬ì‚¬ìš©

**ì‚­ì œ**: `run_training_pipeline()` - Stage 1â†’2 ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë¶ˆí•„ìš”

---

## Part 6: Checkpoint Handoff ë©”ì»¤ë‹ˆì¦˜

### 6.1 save_checkpoint() êµ¬ì¡°

**íŒŒì¼ ê²½ë¡œ**: `src/weighted_mtp/pipelines/checkpoint_utils.py`

**ì—­í• **: Checkpoint ì €ì¥ (Local + MLflow artifact)

**êµ¬í˜„**:

```python
"""Checkpoint ì €ì¥/ë¡œë“œ ìœ í‹¸ë¦¬í‹°

MLflow artifact handoff ì§€ì›
"""

from pathlib import Path
import torch
import logging

logger = logging.getLogger(__name__)


def save_checkpoint(
    adapter,
    optimizer,
    epoch: int,
    train_metrics: dict,
    val_metrics: dict,
    checkpoint_path: Path,
):
    """Checkpoint ì €ì¥

    Args:
        adapter: MetaLlamaMTPAdapter
        optimizer: torch.optim.Optimizer
        epoch: Current epoch
        train_metrics: Training metrics
        val_metrics: Validation metrics
        checkpoint_path: ì €ì¥ ê²½ë¡œ
    """
    checkpoint_path.parent.mkdir(parents=True, exist_ok=True)

    checkpoint = {
        "epoch": epoch,
        "adapter_state_dict": adapter.state_dict(),
        "value_head_state_dict": adapter.value_head.state_dict(),  # Stage 2ì—ì„œ ë¡œë“œìš©
        "optimizer_state_dict": optimizer.state_dict(),
        "train_metrics": train_metrics,
        "val_metrics": val_metrics,
    }

    torch.save(checkpoint, checkpoint_path)
    logger.info(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")
    logger.info(f"   Train loss: {train_metrics.get('stage1_loss', train_metrics.get('stage2_total_loss', 'N/A')):.4f}")
    logger.info(f"   Val loss: {val_metrics['val_loss']:.4f}")
```

**ì €ì¥ ë‚´ìš©**:
- `adapter_state_dict`: ì „ì²´ adapter (Stage 2 final checkpointìš©)
- `value_head_state_dict`: Value headë§Œ (Stage 2 ì´ˆê¸°í™”ìš©)
- `optimizer_state_dict`: Resume trainingìš©
- `train_metrics`, `val_metrics`: ì„±ëŠ¥ ì¶”ì 

### 6.2 load_critic_checkpoint() êµ¬ì¡°

**ì—­í• **: Critic checkpoint ë¡œë“œ (Stage 2ì—ì„œ ì‚¬ìš©)

**êµ¬í˜„**:

```python
def load_critic_checkpoint(checkpoint_path: str, adapter, device):
    """Critic checkpoint ë¡œë“œ (Stage 2ì—ì„œ ì‚¬ìš©)

    Args:
        checkpoint_path: Local path or MLflow artifact URI
        adapter: MetaLlamaMTPAdapter
        device: torch.device

    Returns:
        checkpoint: Loaded checkpoint dict
    """
    # MLflow artifact URI ê°ì§€
    if checkpoint_path.startswith("mlflow://"):
        logger.info(f"Downloading MLflow artifact: {checkpoint_path}")
        import mlflow
        local_path = mlflow.artifacts.download_artifacts(checkpoint_path)
        checkpoint = torch.load(local_path, map_location=device)
    else:
        # Local path
        checkpoint = torch.load(checkpoint_path, map_location=device)

    # Value head state dict ë¡œë“œ
    adapter.value_head.load_state_dict(checkpoint["value_head_state_dict"])
    logger.info(f"âœ… Critic checkpoint loaded: {checkpoint_path}")
    logger.info(f"   Epoch: {checkpoint['epoch']}")
    logger.info(f"   Val loss: {checkpoint['val_metrics']['val_loss']:.4f}")

    return checkpoint
```

**ì§€ì› ê²½ë¡œ í˜•ì‹**:
1. **Local path**: `storage/checkpoints/critic/critic-pretrain/checkpoint_best.pt`
2. **MLflow artifact URI**: `mlflow://8/{run_id}/artifacts/checkpoints/checkpoint_best.pt`

### 6.3 MLflow Artifact ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œ

**ì—…ë¡œë“œ** (run_critic.py):

```python
# Best checkpointë¥¼ MLflow artifactë¡œ ì—…ë¡œë“œ
mlflow.log_artifact(str(best_checkpoint_path), "checkpoints")

# Artifact URI íšë“ (ìë™)
# mlflow://8/{run_id}/artifacts/checkpoints/checkpoint_best.pt
```

**ë‹¤ìš´ë¡œë“œ** (run_verifiable.py):

```python
# Configì— MLflow URI ëª…ì‹œ
experiment:
  critic_checkpoint: mlflow://8/abc123/artifacts/checkpoints/checkpoint_best.pt

# load_critic_checkpoint()ê°€ ìë™ ê°ì§€ ë° ë‹¤ìš´ë¡œë“œ
checkpoint = load_critic_checkpoint(config.experiment.critic_checkpoint, adapter, device)
```

---

## Part 7: Stepë³„ êµ¬í˜„ ê°€ì´ë“œ

### Step 0: ê¸°ì¡´ ì½”ë“œ ì •ë¦¬

**ëª©í‘œ**: ë¶ˆí•„ìš”í•œ íŒŒì¼ ì‚­ì œ ë° ë°±ì—…

**ì‘ì—…**:
```bash
# 1. ê¸°ì¡´ Phase 6 ê³„íšì„œ ì‚­ì œ
rm docs/08_phase6_detailed_plan.md

# 2. ê¸°ì¡´ run_training_pipeline() ì£¼ì„ ì²˜ë¦¬ (í…ŒìŠ¤íŠ¸ í›„ ì‚­ì œ)
# src/weighted_mtp/pipelines/training.py:run_training_pipeline()

# 3. Git commit
git add docs/07_phase5_detailed_plan.md docs/08_phase6_detailed_plan.md
git commit -m "docs: Phase 5 ì¬ì„¤ê³„ - Stageë³„ ë…ë¦½ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸"
```

**ê²€ì¦ ê¸°ì¤€**:
- [ ] Phase 6 ê³„íšì„œ ì‚­ì œ í™•ì¸
- [ ] Phase 5 ê³„íšì„œ ì—…ë°ì´íŠ¸ í™•ì¸

### Step 1-3: Value Weighting ëª¨ë“ˆ (ê¸°ì¡´ ìœ ì§€)

**Step 1**: `value_weighting/td_error.py:compute_td_errors()` - ë³€ê²½ ì—†ìŒ
**Step 2**: `value_weighting/weight_builder.py:build_weights()` - ë³€ê²½ ì—†ìŒ
**Step 3**: `value_weighting/metrics.py:compute_td_stats(), compute_weight_stats()` - ë³€ê²½ ì—†ìŒ

**ê²€ì¦**: ê¸°ì¡´ Unit test í†µê³¼ í™•ì¸
```bash
uv run pytest tests/unit/test_td_error.py -v
uv run pytest tests/unit/test_weight_builder.py -v
uv run pytest tests/unit/test_metrics.py -v
```

### Step 4: checkpoint_utils.py êµ¬í˜„

**ëª©í‘œ**: Checkpoint save/load ìœ í‹¸ë¦¬í‹° êµ¬í˜„

**íŒŒì¼ ìƒì„±**: `src/weighted_mtp/pipelines/checkpoint_utils.py`

**êµ¬í˜„ ë‚´ìš©**:
1. `save_checkpoint()` í•¨ìˆ˜
2. `load_critic_checkpoint()` í•¨ìˆ˜
3. MLflow artifact URI ì§€ì›

**ê²€ì¦ ê¸°ì¤€**:
- [ ] `save_checkpoint()` ì •ìƒ ë™ì‘ (local path ì €ì¥)
- [ ] `load_critic_checkpoint()` local path ë¡œë“œ ì„±ê³µ
- [ ] `load_critic_checkpoint()` MLflow URI ë¡œë“œ ì„±ê³µ (mock test)
- [ ] Unit test ì‘ì„± (`tests/unit/test_checkpoint_utils.py`)

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 2-3ì‹œê°„

### Step 5: Config íŒŒì¼ ìƒì„±

**ëª©í‘œ**: Stageë³„ config íŒŒì¼ ìƒì„±

**ì‘ì—…**:

```bash
# 1. ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p configs/critic
mkdir -p configs/verifiable
mkdir -p configs/rho1

# 2. Config íŒŒì¼ ìƒì„±
touch configs/critic/critic.yaml
touch configs/critic/critic_local.yaml
touch configs/verifiable/verifiable.yaml
touch configs/verifiable/verifiable_local.yaml
touch configs/rho1/rho1.yaml
touch configs/rho1/rho1_local.yaml
```

**ë‚´ìš© ì‘ì„±**: Part 4ì˜ ì˜ˆì‹œ config ë³µì‚¬

**ê²€ì¦ ê¸°ì¤€**:
- [ ] `configs/critic/critic.yaml` ìƒì„± í™•ì¸
- [ ] `configs/verifiable/verifiable.yaml` ìƒì„± í™•ì¸ (critic_checkpoint ê²½ë¡œ í¬í•¨)
- [ ] `configs/rho1/rho1.yaml` ìƒì„± í™•ì¸ (ref model í¬í•¨)
- [ ] OmegaConfë¡œ ë¡œë”© ê°€ëŠ¥ í™•ì¸

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 1-2ì‹œê°„

### Step 6: run_critic.py êµ¬í˜„

**ëª©í‘œ**: Critic pre-training runner êµ¬í˜„

**íŒŒì¼ ìƒì„±**: `src/weighted_mtp/pipelines/run_critic.py`

**êµ¬í˜„ ë‚´ìš©**: Part 5.1ì˜ êµ¬ì¡° ì°¸ê³ 

**í•µì‹¬ ë¡œì§**:
1. Config merge (defaults + critic)
2. MLflow run ìƒì„±
3. Resource ë¡œë”© (adapter, tokenizer, datasets)
4. Training loop (train_stage1 í˜¸ì¶œ)
5. Checkpoint ì €ì¥ (best, final)
6. MLflow artifact ì—…ë¡œë“œ

**ê²€ì¦ ê¸°ì¤€**:
- [ ] `python -m weighted_mtp.pipelines.run_critic --config configs/critic/critic_local.yaml` ì‹¤í–‰ ì„±ê³µ
- [ ] Checkpoint ì €ì¥ í™•ì¸ (`storage/checkpoints/critic/.../checkpoint_best.pt`)
- [ ] MLflow run ìƒì„± í™•ì¸ (http://13.50.240.176)
- [ ] Artifact ì—…ë¡œë“œ í™•ì¸ (MLflow UI)

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 3-4ì‹œê°„

### Step 7: run_verifiable.py êµ¬í˜„

**ëª©í‘œ**: Verifiable WMTP runner êµ¬í˜„

**íŒŒì¼ ìƒì„±**: `src/weighted_mtp/pipelines/run_verifiable.py`

**êµ¬í˜„ ë‚´ìš©**: Part 5.2ì˜ êµ¬ì¡° ì°¸ê³ 

**í•µì‹¬ ë¡œì§**:
1. Config merge
2. Critic checkpoint ê²½ë¡œ ê²€ì¦
3. `load_critic_checkpoint()` í˜¸ì¶œ
4. Training loop (train_stage2 í˜¸ì¶œ)
5. Checkpoint ì €ì¥

**ê²€ì¦ ê¸°ì¤€**:
- [ ] Critic checkpoint ë¡œë“œ ì„±ê³µ
- [ ] `python -m weighted_mtp.pipelines.run_verifiable --config configs/verifiable/verifiable_local.yaml --critic-checkpoint <path>` ì‹¤í–‰ ì„±ê³µ
- [ ] MLflow run ìƒì„± í™•ì¸ (parent run ì—°ê²°)
- [ ] Checkpoint ì €ì¥ í™•ì¸

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 3-4ì‹œê°„

### Step 8: run_rho1.py êµ¬í˜„

**ëª©í‘œ**: Rho-1 weighted training runner êµ¬í˜„

**íŒŒì¼ ìƒì„±**: `src/weighted_mtp/pipelines/run_rho1.py`

**êµ¬í˜„ ë‚´ìš©**:
1. Policy + Reference model ë¡œë”©
2. Excess loss ê³„ì‚° ë¡œì§
3. Rho-1 weight ê³„ì‚° ë¡œì§
4. Training loop

**ê²€ì¦ ê¸°ì¤€**:
- [ ] Ref model ë¡œë“œ ì„±ê³µ
- [ ] Excess loss ê³„ì‚° ì •í™•
- [ ] `python -m weighted_mtp.pipelines.run_rho1 --config configs/rho1/rho1_local.yaml` ì‹¤í–‰ ì„±ê³µ

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 4-5ì‹œê°„

### Step 9: ê¸°ì¡´ ì½”ë“œ ì‚­ì œ ë° ì •ë¦¬

**ëª©í‘œ**: ë¶ˆí•„ìš”í•œ ì½”ë“œ ì œê±°

**ì‚­ì œ ëŒ€ìƒ**:
1. `src/weighted_mtp/pipelines/training.py:run_training_pipeline()` í•¨ìˆ˜
2. `src/weighted_mtp/cli/train.py` (ë˜ëŠ” ì£¼ì„ ì²˜ë¦¬)

**ê²€ì¦ ê¸°ì¤€**:
- [ ] ëª¨ë“  runner ë…ë¦½ ì‹¤í–‰ í™•ì¸
- [ ] ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ (run_training_pipeline ì œê±°)
- [ ] Linting í†µê³¼ (`ruff check --fix`)

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 1-2ì‹œê°„

---

## Part 8: ê²€ì¦ ë° ì™„ë£Œ ê¸°ì¤€

### 8.1 ê¸°ëŠ¥ ê²€ì¦

**Critic Pre-training (Stage 1)**:
- [ ] `python -m weighted_mtp.pipelines.run_critic --config configs/critic/critic_local.yaml` ì‹¤í–‰ ì„±ê³µ
- [ ] Checkpoint ì €ì¥ í™•ì¸ (`storage/checkpoints/critic/.../checkpoint_best.pt`)
- [ ] MLflow run ìƒì„± í™•ì¸ (tags: critic, stage1)
- [ ] Artifact ì—…ë¡œë“œ í™•ì¸ (MLflow UI)

**Verifiable WMTP (Stage 2)**:
- [ ] Critic checkpoint ë¡œë“œ ì„±ê³µ
- [ ] `python -m weighted_mtp.pipelines.run_verifiable --config configs/verifiable/verifiable_local.yaml --critic-checkpoint <path>` ì‹¤í–‰ ì„±ê³µ
- [ ] MLflow run ìƒì„± í™•ì¸ (tags: verifiable, stage2)
- [ ] Parent run ì—°ê²° í™•ì¸ (ì„ íƒì )

**Rho-1 Training**:
- [ ] Ref model ë¡œë“œ ì„±ê³µ
- [ ] `python -m weighted_mtp.pipelines.run_rho1 --config configs/rho1/rho1_local.yaml` ì‹¤í–‰ ì„±ê³µ
- [ ] Excess loss ê³„ì‚° ì •í™•
- [ ] MLflow run ìƒì„± í™•ì¸ (tags: rho1)

**Checkpoint Handoff**:
- [ ] Local path checkpoint ë¡œë“œ ì„±ê³µ
- [ ] MLflow artifact URI checkpoint ë¡œë“œ ì„±ê³µ
- [ ] Value head state dict ì •í™•íˆ ë¡œë“œë¨

### 8.2 ì„±ëŠ¥ ê²€ì¦

**ì¬ì‹¤í–‰ íš¨ìœ¨**:
- [ ] Stage 1 ì‹¤í–‰ ì‹œê°„ ì¸¡ì • (ì˜ˆ: 10ë¶„)
- [ ] Stage 2ë§Œ ì¬ì‹¤í–‰ ì‹œ Stage 1 skip í™•ì¸ (ì˜ˆ: 5ë¶„ ì ˆì•½)

**MLflow ì¶”ì **:
- [ ] Critic runê³¼ Verifiable run ë¶„ë¦¬ í™•ì¸
- [ ] Metrics ì •í™•íˆ ë¡œê¹…ë¨ (train/loss, val/loss)
- [ ] Artifacts ì •í™•íˆ ì—…ë¡œë“œë¨ (checkpoints/)

### 8.3 ì½”ë“œ í’ˆì§ˆ ê²€ì¦

**Linting**:
```bash
uv run ruff check --fix src/weighted_mtp/pipelines/
```

**Type checking** (ì„ íƒì ):
```bash
uv run mypy src/weighted_mtp/pipelines/
```

**Unit tests**:
```bash
uv run pytest tests/unit/test_checkpoint_utils.py -v
uv run pytest tests/unit/test_td_error.py -v
uv run pytest tests/unit/test_weight_builder.py -v
uv run pytest tests/unit/test_metrics.py -v
```

### 8.4 ì™„ë£Œ ê¸°ì¤€

**í•„ìˆ˜ (Must-have)**:
- [x] Config ë¶„ë¦¬ êµ¬ì¡° ì™„ì„± (critic/, verifiable/, rho1/)
- [x] `run_critic.py` êµ¬í˜„ ë° ë…ë¦½ ì‹¤í–‰ ì„±ê³µ
- [x] `run_verifiable.py` êµ¬í˜„ ë° checkpoint ë¡œë“œ ì„±ê³µ
- [x] `checkpoint_utils.py` êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸ í†µê³¼
- [x] MLflow artifact handoff ë™ì‘ í™•ì¸
- [x] ê¸°ì¡´ `run_training_pipeline()` ì‚­ì œ

**ê¶Œì¥ (Should-have)**:
- [ ] `run_rho1.py` êµ¬í˜„ (Rho-1 ì‹¤í—˜ ì§€ì›)
- [ ] MLflow Projects entry_points ì •ì˜ (`MLproject` íŒŒì¼)
- [ ] Integration test ì‘ì„± (end-to-end)

**ì„ íƒì  (Nice-to-have)**:
- [ ] Parent-child run ì—°ê²° (MLflow UIì—ì„œ workflow ì¶”ì )
- [ ] Checkpoint versioning (checkpoint_v1.pt, checkpoint_v2.pt)
- [ ] Distributed training ì§€ì› (Rank 0 only operations)

---

## Part 9: ì˜ˆìƒ ì†Œìš” ì‹œê°„

| ì‘ì—… | ì˜ˆìƒ ì‹œê°„ | ë¹„ê³  |
|------|-----------|------|
| Step 0: ê¸°ì¡´ ì½”ë“œ ì •ë¦¬ | 0.5ì‹œê°„ | Phase 6 ê³„íšì„œ ì‚­ì œ |
| Step 1-3: Value Weighting (ê¸°ì¡´) | 0ì‹œê°„ | ë³€ê²½ ì—†ìŒ |
| Step 4: checkpoint_utils.py | 2-3ì‹œê°„ | save/load í•¨ìˆ˜ + tests |
| Step 5: Config íŒŒì¼ ìƒì„± | 1-2ì‹œê°„ | 6ê°œ YAML íŒŒì¼ ì‘ì„± |
| Step 6: run_critic.py | 3-4ì‹œê°„ | Stage 1 runner êµ¬í˜„ |
| Step 7: run_verifiable.py | 3-4ì‹œê°„ | Stage 2 runner êµ¬í˜„ |
| Step 8: run_rho1.py | 4-5ì‹œê°„ | Rho-1 runner êµ¬í˜„ (ì„ íƒì ) |
| Step 9: ê¸°ì¡´ ì½”ë“œ ì‚­ì œ | 1-2ì‹œê°„ | ì •ë¦¬ ë° í…ŒìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ |
| í†µí•© í…ŒìŠ¤íŠ¸ ë° ë””ë²„ê¹… | 3-4ì‹œê°„ | End-to-end ê²€ì¦ |
| ë¬¸ì„œí™” ë° ìµœì¢… ê²€í†  | 1-2ì‹œê°„ | README ì—…ë°ì´íŠ¸ |
| **í•©ê³„ (Rho-1 ì œì™¸)** | **14-21ì‹œê°„** | ì•½ 2-3ì¼ |
| **í•©ê³„ (Rho-1 í¬í•¨)** | **18-26ì‹œê°„** | ì•½ 2.5-3.5ì¼ |

---

## Part 10: ë‹¤ìŒ ë‹¨ê³„ (Phase 5 ì™„ë£Œ í›„)

**Phase 5 ì™„ë£Œ ê¸°ì¤€ ì¶©ì¡± ì‹œ**:
- âœ… Critic / Verifiable ë…ë¦½ ì‹¤í–‰ ê°€ëŠ¥
- âœ… Checkpoint handoff ë™ì‘ í™•ì¸
- âœ… MLflow ì‹¤í—˜ ì¶”ì  ê°€ëŠ¥
- âœ… Config ë¶„ë¦¬ ì™„ë£Œ

**ë‹¤ìŒ ì‘ì—…**:
1. **Production ì‹¤í—˜ ì‹¤í–‰**: Critic + Verifiable full training
2. **ì„±ëŠ¥ ë¶„ì„**: MLflow UIì—ì„œ metrics ë¹„êµ (baseline vs. verifiable)
3. **Rho-1 ì‹¤í—˜**: Ref model ê¸°ë°˜ weighted training
4. **ë…¼ë¬¸ ì‘ì„±**: WMTP ì—°êµ¬ ê²°ê³¼ ì •ë¦¬

**ì„ íƒì  ê°œì„ **:
- Distributed training ì§€ì› (DDP)
- Hyperparameter tuning (Ray Tune ì—°ë™)
- Automated pipeline (Airflow/Prefect)

---

## ë¶€ë¡ A: MLflow Projects í†µí•© (ì„ íƒì )

**MLproject íŒŒì¼ ìƒì„±**:

```yaml
# MLproject
name: weighted-mtp

entry_points:
  critic_training:
    parameters:
      config: {type: str, default: configs/critic/critic.yaml}
    command: "python -m weighted_mtp.pipelines.run_critic --config {config}"

  verifiable_training:
    parameters:
      config: {type: str, default: configs/verifiable/verifiable.yaml}
      critic_checkpoint: {type: str}
    command: "python -m weighted_mtp.pipelines.run_verifiable --config {config} --critic-checkpoint {critic_checkpoint}"

  rho1_training:
    parameters:
      config: {type: str, default: configs/rho1/rho1.yaml}
    command: "python -m weighted_mtp.pipelines.run_rho1 --config {config}"
```

**ì‹¤í–‰**:

```bash
# MLflow Projectsë¡œ ì‹¤í–‰
mlflow run . -e critic_training -P config=configs/critic/critic.yaml

mlflow run . -e verifiable_training \
    -P config=configs/verifiable/verifiable.yaml \
    -P critic_checkpoint=storage/checkpoints/critic/.../checkpoint_best.pt
```

---

## ë¶€ë¡ B: ê°œë°œì›ì¹™ ì¤€ìˆ˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [x] **ì›ì¹™ 1**: Phase 4 â†’ Phase 5 íë¦„ ë¶„ì„ ì™„ë£Œ (Adapter â†’ Stageë³„ Runner)
- [x] **ì›ì¹™ 2**: ê¸°ì¡´ êµ¬ì¡° ì¡´ì¤‘ (train_stage1/2, value_weighting ì¬ì‚¬ìš©), ì¤‘ë³µ ì œê±° (run_training_pipeline ì‚­ì œ)
- [x] **ì›ì¹™ 3**: ì˜ëª»ëœ êµ¬ì¡° ì „ê²© ì‚­ì œ (ë‹¨ì¼ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ íŒŒì´í”„ë¼ì¸ íê¸°)
- [x] **ì›ì¹™ 4**: í•˜ìœ„ í˜¸í™˜ì„± ê³ ë ¤ ì•ˆ í•¨ (ì™„ì „íˆ ìƒˆë¡œìš´ Stage ë¶„ë¦¬ êµ¬ì¡°)
- [x] **ì›ì¹™ 4-1**: ì¸ìëª… í†µì¼ (config, checkpoint_path, device)
- [x] **ì›ì¹™ 4-2**: Wrapper ìµœì†Œí™” (runnerëŠ” í•„ìˆ˜ì  entry point)
- [x] **ì›ì¹™ 4-3**: í•œê¸€ ì£¼ì„, ì´ëª¨ì§€ ì—†ìŒ, ë²„ì „ë³„ ì£¼ì„ ì œê±°
- [ ] **ì›ì¹™ 5**: ê³„íšì„œì™€ ì½”ë“œ ì¼ì¹˜ ì—¬ë¶€ ìµœì¢… ê²€í†  (êµ¬í˜„ í›„)
- [x] **ì›ì¹™ 6**: ì˜ì¡´ì„± ë„êµ¬ í™œìš© (MLflow, OmegaConf, torch)

---

**ë¬¸ì„œ ì¢…ë£Œ**
