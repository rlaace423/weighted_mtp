# Critic Pre-training (Stage 1) - Pairwise Ranking Loss
# Shortcut Learning 방지를 위한 동일 instruction 내 상대 비교 학습
# 프로덕션 설정 (VESSL H200 4-GPU)

# 프로젝트 정보
project:
  name: weighted-mtp
  version: "2.0.0"

# 실험 메타정보
experiment:
  name: critic-pretrain-pairwise
  description: "Pairwise ranking loss for relative comparison within same instruction"
  stage: critic
  tags:
    - critic
    - pairwise-ranking
    - mlp-value-head
    - dropout-0.3

# 모델 설정
models:
  policy:
    name: meta-llama-mtp
    path: storage/models/meta-llama-mtp
    tokenizer_path: storage/models/meta-llama-mtp/tokenizer
    params:
      dim: 4096
      n_layers: 32
      n_heads: 32
      n_future_tokens: 4
      intermediate_size: 11008
      rope_theta: 10000.0
      vocab_size: 32000
    dtype: bfloat16

# 데이터셋 설정
dataset:
  name: codecontests
  train: storage/datasets/codecontests/processed/train.jsonl
  validation: storage/datasets/codecontests/processed/valid.jsonl
  max_length: 2048

# 데이터 샘플링 (Pairwise)
data_sampling:
  seed: 42
  val_n_samples: 500
  use_pairwise: true

  n_samples: 50000  # 학습용 (correct, incorrect) 쌍
  max_pairs_per_problem: 15  # problem당 최대 쌍 수 (다양성 확보, 과적합 방지)

  difficulty_bins:
    zero: [0, 0]
    diff_7: [7, 7]
    else: [8, 25]
  difficulty_weights:
    zero: 0.4
    diff_7: 0.3
    else: 0.3

# 학습 설정 (H200 4-GPU 최적화)
training:
  n_epochs: 2.0
  batch_size: 64  # Pairwise는 메모리 2배 소모 (pos + neg)
  gradient_accumulation_steps: 1
  trunk_learning_rate: 0
  value_head_learning_rate: 5.0e-4

  # 컴포넌트별 gradient clipping (trunk과 value_head 독립 적용)
  # Critic은 trunk frozen이므로 value_head만 clipping 적용됨
  trunk_max_grad_norm: 1.0       # Trunk gradient clipping (frozen 시 무의미)
  value_head_max_grad_norm: 5.0  # Value head gradient clipping
  max_grad_norm: 1.0             # fallback (컴포넌트별 설정 없을 때)

  num_unfrozen_layers: 0
  value_head_type: mlp
  dropout: 0.1  # Shortcut Learning 방지
  trunk_frozen: true  # true면 value loss gradient가 trunk로 흐르지 않음
  log_interval: 1

  # Value loss 모드 선택 (pairwise와 mc_mse 독립적 또는 동시 사용 가능)
  value_loss:
    use_pairwise: true   # Pairwise ranking loss
    use_mc_mse: false    # MC tokenwise MSE loss
    pairwise_coef: 1.0   # Pairwise loss 계수
    mc_mse_coef: 1.0     # MC MSE loss 계수

  # LoRA 설정 (선택적)
  # LoRA 설정 (Critic은 trunk frozen으로 기본 비활성화)
  use_lora: false
  lora:
    rank: 64           # 활성화 시 사용할 설정
    alpha: 128.0
    dropout: 0.05
    target_modules:
      - wq
      - wk
      - wv
      - wo
      - w1
      - w2
      - w3

  # Learning rate scheduler
  lr_scheduler:
    type: cosine
    warmup_ratio: 0.03
    min_lr_ratio: 0.01

# 체크포인트
checkpoint:
  save_dir: storage/checkpoints/critic/${experiment.name}
  save_checkpoint_every: 0.25
  save_best: true
  save_final: true
  save_total_limit: 3
  s3_upload: true
  save_lora_only: false

# 런타임 설정
runtime:
  device: cuda
  seed: 42
  mixed_precision: true

# 분산학습 설정
distributed:
  fsdp:
    sharding_strategy: FULL_SHARD
    mixed_precision: true
    cpu_offload: false
    activation_checkpointing: true

# 스토리지
storage:
  root: storage
  models_dir: storage/models
  datasets_dir: storage/datasets
  checkpoints_dir: storage/checkpoints

# MLflow 추적
mlflow:
  tracking_uri: ""
  experiment: "weighted-mtp/production"

# 로깅
logging:
  level: INFO
  format: "%(asctime)s [%(levelname)s] [%(name)s] %(message)s"
